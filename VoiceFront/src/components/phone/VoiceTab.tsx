// components/phone/VoiceTab.tsx
"use client";

import React, { useState, useRef, useEffect } from "react";
import Button from "@/components/ui/Button";
import Card from "@/components/ui/Card";
import { authService, MeResponse } from "../../services/api/auth";

interface Room {
  id: string;
  name: string;
  participants: number;
}

const VoiceTab: React.FC = () => {
  const [sttLogs, setSttLogs] = useState<string[]>([]);
  const [aiLogs, setAiLogs] = useState<string[]>([]);
  const [rooms, setRooms] = useState<Room[]>([]);
  const [currentRoom, setCurrentRoom] = useState<string | null>(null);
  const [userEmail, setUserEmail] = useState<string | null>(null);
  const [activeTab, setActiveTab] = useState<"stt" | "ai">("stt");

  const wsRef = useRef<WebSocket | null>(null); // AI ë¶„ì„ WS
  const callWsRef = useRef<WebSocket | null>(null); // í†µí™”ìš© WS
  const audioCtxRef = useRef<AudioContext | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const recordingRef = useRef(false);
  const callBufferRef = useRef<Int16Array[]>([]); // ë‚´ìŒì„± í†µí™”ë¡œ ì „ì†¡
  const aiMyBufferRef = useRef<Int16Array[]>([]); // ë‚´ ìŒì„±ëˆ„ì 
  const peerBufferRef = useRef<Int16Array[]>([]); // ìƒëŒ€ ìŒì„±ëˆ„ì 
  const fileInputRef = useRef<HTMLInputElement | null>(null);
  const intervalRef = useRef<number | null>(null);
  const sessionIdRef = useRef(`s_${Date.now()}`);
  const historyRef = useRef<{ role: string; content: string }[]>([]);
  const vizLastUpdateRef = useRef<number>(0);
  // ìµœê·¼ spoof_prob ê¸°ë¡ (ìµœëŒ€ 10ê°œ ìœ ì§€)
  const spoofHistoryRef = useRef<number[]>([]);
  const [vizLevels, setVizLevels] = useState<number[]>(
    Array.from({ length: 16 }, () => 0)
  );

  const wsUrl = process.env.NEXT_PUBLIC_WS_URL!;
  const callWsUrl = process.env.NEXT_PUBLIC_CALL_WS!;
  const AI_SERVER_URL = process.env.NEXT_PUBLIC_AI_SERVER_URL!;
  const CALL_API_URL = process.env.NEXT_PUBLIC_API_URL!; // /call API ì„œë²„
  const API_TOKEN = process.env.NEXT_PUBLIC_API_TOKEN!;

  const logSTT = (msg: string) => setSttLogs((prev) => [...prev, msg]);
  const logAI = (msg: string) => setAiLogs((prev) => [...prev, msg]);

  /** ---------------- Helper ---------------- */
  const triggerFileSelect = () => fileInputRef.current?.click();

  /** ---------------- WAV ì¸ì½”ë”© ---------------- */
  const encodeWAV = (samples: Int16Array, sampleRate: number) => {
    const buffer = new ArrayBuffer(44 + samples.length * 2);
    const view = new DataView(buffer);

    const writeString = (view: DataView, offset: number, str: string) => {
      for (let i = 0; i < str.length; i++)
        view.setUint8(offset + i, str.charCodeAt(i));
    };

    writeString(view, 0, "RIFF");
    view.setUint32(4, 36 + samples.length * 2, true);
    writeString(view, 8, "WAVE");
    writeString(view, 12, "fmt ");
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true);
    view.setUint16(22, 1, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * 2, true);
    view.setUint16(32, 2, true);
    view.setUint16(34, 16, true);
    writeString(view, 36, "data");
    view.setUint32(40, samples.length * 2, true);

    let offset = 44;
    for (let i = 0; i < samples.length; i++, offset += 2)
      view.setInt16(offset, samples[i], true);

    return new Blob([view], { type: "audio/wav" });
  };

  /** ---------------- AI WS ---------------- */
  const connectWS = () => {
    if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) return;
    wsRef.current = new WebSocket(wsUrl);

    wsRef.current.onopen = () => logSTT("âœ… AI WebSocket connected");
    wsRef.current.onmessage = (event) => {
      try {
        const data = JSON.parse(event.data);
        if (data.stt_segment) {
          const speaker = data.stt_segment.speaker.replace(
            "SPEAKER_",
            "ëŒ€í™”ì"
          );
          const text = data.stt_segment.text;
          logSTT(`${speaker}: ${text}`);

          // âš¡ STT â†’ AI ì„œë²„ POST
          sendSTTToAI({ speaker, text });
        } else if (data.spoof_prob !== undefined) {
          const prob = data.spoof_prob;
          const prob2 = 1 - prob;
          console.log(prob2 * 100);
          spoofHistoryRef.current.push(prob2);
          if (spoofHistoryRef.current.length > 10) {
            spoofHistoryRef.current.shift();
          }
          const aboveThreshold = spoofHistoryRef.current.filter(
            (p) => p >= 0.5
          );
          if (aboveThreshold.length >= 4) {
            const avgProb =
              aboveThreshold.reduce((sum, p) => sum + p, 0) /
              aboveThreshold.length;
            logSTT(
              `ğŸ›‘ì£¼ì˜! ë³€ì¡°ìŒì„± ê°ì§€!\nê°€ëŠ¥ì„±: ${(avgProb * 100).toFixed(1)}%`
            );
          }
        } else if (data.error) logSTT(`âŒ Error: ${data.error}`);
      } catch {
        logSTT(`ğŸ“© Raw message: ${event.data}`);
      }
    };
    wsRef.current.onclose = () => logSTT("âš ï¸ AI WebSocket disconnected");
  };

  /** ---------------- í†µí™”ìš© WS ---------------- */
  const connectCallWS = (roomId: string) => {
    if (callWsRef.current && callWsRef.current.readyState === WebSocket.OPEN)
      return;

    const url = `${callWsUrl}?roomId=${roomId}`;
    callWsRef.current = new WebSocket(url);

    callWsRef.current.onopen = () =>
      logSTT(`âœ… Call WebSocket connected (room: ${roomId})`);

    callWsRef.current.onerror = (err) =>
      logSTT(`âŒ Call WS error: ${JSON.stringify(err)}`);

    callWsRef.current.onmessage = async (evt) => {
      const arrayBuffer = await (evt.data as Blob).arrayBuffer();
      if (!audioCtxRef.current) audioCtxRef.current = new AudioContext();

      try {
        const audioBuffer = await audioCtxRef.current.decodeAudioData(
          arrayBuffer
        );
        const source = audioCtxRef.current.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(audioCtxRef.current.destination);
        source.start();

        // ğŸ¯ ìƒëŒ€ë°© ìŒì„±ì„ Int16Arrayë¡œ ë³€í™˜ í›„ AI ì „ì†¡ ë²„í¼ì— ì¶”ê°€
        const channelData = audioBuffer.getChannelData(0);
        const int16 = new Int16Array(channelData.length);
        for (let i = 0; i < channelData.length; i++) {
          int16[i] = Math.max(-1, Math.min(1, channelData[i])) * 0x7fff;
        }
        peerBufferRef.current.push(int16);
      } catch (e) {
        console.error("ì¬ìƒ ì‹¤íŒ¨, raw PCMì¼ ìˆ˜ ìˆìŒ:", e);
        const pcm = new Int16Array(arrayBuffer);
        playPCM(pcm);
        peerBufferRef.current.push(pcm); // raw PCM fallbackë„ AI WSì— ì¶”ê°€
      }
    };
    callWsRef.current.onclose = (evt) =>
      logSTT(
        `âš ï¸ Call WebSocket disconnected (code: ${evt.code}, reason: ${evt.reason})`
      );
  };

  /** ---------------- ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¬ë° ---------------- */
  // âœ… AI WS ì „ì†¡ (5ì´ˆ ì£¼ê¸°, ë‚´/ìƒëŒ€ ìŒì„± ë³„ë„ ì „ì†¡)
  const startAIInterval = () => {
    intervalRef.current = window.setInterval(async () => {
      if (!recordingRef.current) return;

      // --------- ë‚´ ìŒì„± ì „ì†¡ ---------
      if (
        aiMyBufferRef.current.length > 0 &&
        wsRef.current?.readyState === WebSocket.OPEN
      ) {
        const myLength = aiMyBufferRef.current.reduce(
          (acc, cur) => acc + cur.length,
          0
        );
        const myMerged = new Int16Array(myLength);
        let offset = 0;
        aiMyBufferRef.current.forEach((chunk) => {
          myMerged.set(chunk, offset);
          offset += chunk.length;
        });

        const myBlob = encodeWAV(myMerged, audioCtxRef.current!.sampleRate);
        const myArrayBuffer = await myBlob.arrayBuffer(); // awaitë¡œ ìˆœì°¨ ì²˜ë¦¬
        wsRef.current.send(myArrayBuffer);

        aiMyBufferRef.current = []; // ì´ˆê¸°í™”
      }

      // --------- ìƒëŒ€ ìŒì„± ì „ì†¡ ---------
      if (
        peerBufferRef.current.length > 0 &&
        wsRef.current?.readyState === WebSocket.OPEN
      ) {
        const peerLength = peerBufferRef.current.reduce(
          (acc, cur) => acc + cur.length,
          0
        );
        const peerMerged = new Int16Array(peerLength);
        let offset = 0;
        peerBufferRef.current.forEach((chunk) => {
          peerMerged.set(chunk, offset);
          offset += chunk.length;
        });

        const peerBlob = encodeWAV(peerMerged, audioCtxRef.current!.sampleRate);
        const peerArrayBuffer = await peerBlob.arrayBuffer(); // awaitë¡œ ìˆœì°¨ ì²˜ë¦¬
        wsRef.current.send(peerArrayBuffer);

        peerBufferRef.current = []; // ì´ˆê¸°í™”
      }
    }, 5000);
  };

  // âœ… í†µí™” WS ì „ì†¡ (2ì´ˆ ì£¼ê¸°, ë‚´ ìŒì„±ë§Œ)
  const startCallInterval = () => {
    const callInterval = window.setInterval(() => {
      if (!recordingRef.current) return;
      if (callBufferRef.current.length === 0) return;

      const myLength = callBufferRef.current.reduce(
        (acc, cur) => acc + cur.length,
        0
      );
      const myMerged = new Int16Array(myLength);
      let offset = 0;
      callBufferRef.current.forEach((chunk) => {
        myMerged.set(chunk, offset);
        offset += chunk.length;
      });

      if (callWsRef.current?.readyState === WebSocket.OPEN) {
        callWsRef.current.send(myMerged.buffer);
      }
      callBufferRef.current = [];
    }, 2000); // 2ì´ˆë§ˆë‹¤ ì „ì†¡

    // cleanup ìœ„í•´ refì— ì €ì¥
    (callWsRef as any).interval = callInterval;
  };

  const handleMicStop = () => {
    recordingRef.current = false;
    processorRef.current?.disconnect();
    streamRef.current?.getTracks().forEach((track) => track.stop());
    audioCtxRef.current?.close();

    if (intervalRef.current) clearInterval(intervalRef.current);
    if ((callWsRef as any).interval) clearInterval((callWsRef as any).interval);

    logSTT("ğŸ›‘ Recording stopped");
  };

  /** ---------------- íŒŒì¼ ì—…ë¡œë“œ ---------------- */
  const handleFileUpload = async (e: React.ChangeEvent<HTMLInputElement>) => {
    if (!e.target.files?.length) return;
    const file = e.target.files[0];
    const arrayBuffer = await file.arrayBuffer();

    if (wsRef.current?.readyState === WebSocket.OPEN) {
      const ext = file.name.split(".").pop()?.toLowerCase() || "wav";
      const base64 = btoa(
        new Uint8Array(arrayBuffer).reduce(
          (data, byte) => data + String.fromCharCode(byte),
          ""
        )
      );
      wsRef.current.send(JSON.stringify({ audio: base64, format: ext }));
      logSTT(`ğŸ“¤ Sent file: ${file.name} (format: ${ext})`);
    } else {
      logSTT("âŒ AI WebSocket not connected");
    }
  };

  /** ---------------- í†µí™”ë°© ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° ---------------- */
  const fetchRooms = async () => {
    try {
      const res = await fetch(`${CALL_API_URL}/call/room/list`);
      const data = await res.json();
      // ë°±ì—”ë“œê°€ { rooms: [...] } í˜•íƒœë¼ë©´
      setRooms(data.rooms || []);
    } catch (err) {
      logSTT(`âŒ ë°© ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨: ${(err as Error).message}`);
      setRooms([]); // ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ë¹ˆ ë°°ì—´
    }
  };

  const createRoom = async () => {
    const payload = { creatorId: userEmail, roomName: "ìƒˆë¡œìš´ ë°©" };
    const res = await fetch(`${CALL_API_URL}/call/room/create`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(payload),
    });
    const data = await res.json();
    setCurrentRoom(data.roomId);
    connectCallWS(data.roomId);
    handleMicStart();
    logSTT(`ğŸ“ í†µí™”ë°© ìƒì„±: ${data.roomName} (${data.roomId})`);
    fetchRooms();
  };

  const joinRoom = async (roomId: string) => {
    await fetch(`${CALL_API_URL}/call/room/join`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ userId: userEmail, roomId }),
    });
    setCurrentRoom(roomId);
    connectCallWS(roomId);
    handleMicStart();
    logSTT(`ğŸ“ í†µí™”ë°© ì°¸ê°€: ${roomId}`);
  };

  /** ---------------- PCM ì¬ìƒ ---------------- */
  const playPCM = (pcm: Int16Array, sampleRate = 48000) => {
    if (!audioCtxRef.current) audioCtxRef.current = new AudioContext();

    const float32 = new Float32Array(pcm.length);
    for (let i = 0; i < pcm.length; i++) float32[i] = pcm[i] / 0x7fff;

    const buffer = audioCtxRef.current.createBuffer(
      1,
      float32.length,
      sampleRate // ì„œë²„ì—ì„œ ì†¡ì¶œí•œ sampleRateì— ë§ì¶°ì•¼ í•¨
    );
    buffer.getChannelData(0).set(float32);

    const source = audioCtxRef.current.createBufferSource();
    source.buffer = buffer;
    source.connect(audioCtxRef.current.destination);
    source.start();
  };

  const handleMicStart = async () => {
    if (recordingRef.current) return;
    recordingRef.current = true;

    const stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        sampleRate: 48000,
        channelCount: 1,
        echoCancellation: true, // ì—ì½” ì œê±°
        noiseSuppression: true, // ì¡ìŒ ì–µì œ
        autoGainControl: true, // ìë™ ë³¼ë¥¨ ì¡°ì •
      },
    });
    streamRef.current = stream;

    audioCtxRef.current = new AudioContext({ sampleRate: 48000 });
    const source = audioCtxRef.current.createMediaStreamSource(stream);

    // ğŸ”¥ GainNode ì¶”ê°€
    const gainNode = audioCtxRef.current.createGain();
    gainNode.gain.value = 0.4; // 0.0 ~ 1.0 (ê¸°ë³¸ 1.0, ì¤„ì´ë©´ ê°ë„ â†“, ì˜¬ë¦¬ë©´ ê°ë„ â†‘)

    processorRef.current = audioCtxRef.current.createScriptProcessor(
      4096,
      1,
      1
    );

    processorRef.current.onaudioprocess = (e) => {
      const input = e.inputBuffer.getChannelData(0);
      const int16 = new Int16Array(input.length);
      let sumSquares = 0;
      for (let i = 0; i < input.length; i++) {
        const clamped = Math.max(-1, Math.min(1, input[i]));
        int16[i] = clamped * 0x7fff;
        sumSquares += clamped * clamped;
      }
      aiMyBufferRef.current.push(int16);
      callBufferRef.current.push(int16);

      // ë¹„ì£¼ì–¼ë¼ì´ì € ì—…ë°ì´íŠ¸(ìµœëŒ€ 60ms ê°„ê²©)
      const now = performance.now();
      if (now - vizLastUpdateRef.current > 60) {
        vizLastUpdateRef.current = now;
        const rms = Math.sqrt(sumSquares / input.length); // 0..1
        const base = Math.min(1, rms * 2.5); // ë¯¼ê°ë„ ë³´ì •
        setVizLevels((prev) =>
          prev.map((_, idx) => {
            const jitter = (Math.sin(now / 120 + idx) + 1) / 12; // 0..~0.16
            const decay = prev[idx] * 0.85; // ë¶€ë“œëŸ¬ìš´ ê°ì‡ 
            return Math.max(decay, Math.min(1, base + jitter));
          })
        );
      }
    };

    source.connect(gainNode);
    gainNode.connect(processorRef.current!);
    processorRef.current.connect(audioCtxRef.current.destination);

    // âœ… ë‘ ê°€ì§€ interval ì‹œì‘
    startAIInterval();
    startCallInterval();

    logSTT("ğŸ¤ Mic started");
  };

  const leaveRoom = async () => {
    if (!currentRoom || !userEmail) {
      logSTT("âŒ ë°© ì •ë³´ê°€ ì—†ì–´ì„œ ë‚˜ê°ˆ ìˆ˜ ì—†ìŒ");
      return;
    }

    try {
      const res = await fetch(`${CALL_API_URL}/call/room/leave`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ userId: userEmail, roomId: currentRoom }),
      });
      const data = await res.json();
      logSTT(`ğŸšª Left room: ${JSON.stringify(data)}`);
      setCurrentRoom(null);
      fetchRooms(); // ë°© ëª©ë¡ ê°±ì‹ 
    } catch (err) {
      logSTT(`âŒ ë°© ë‚˜ê°€ê¸° ì‹¤íŒ¨: ${(err as Error).message}`);
    }
  };

  // ì±—ë´‡ì— ëŒ€í™” ë³´ë‚´ê¸°
  const sendSTTToAI = async (segment: { speaker: string; text: string }) => {
    try {
      const payload = {
        sessionId: sessionIdRef.current, // ê³ ì • ì„¸ì…˜
        query: `
ë‹¹ì‹ ì€ ë³´ì´ìŠ¤í”¼ì‹± íƒì§€ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µí•˜ì„¸ìš”.
ì•„ë˜ ëŒ€í™” ë°œí™”ë¥¼ ë³´ê³  ëŒ€ë‹µí•´ ì£¼ì„¸ìš”.
ëŒ€í™” ë°œí™”ì— ìœ„í—˜ì§•í›„ê°€ ìˆì„ê²½ìš°ì—ë§Œ ì²«ë§ˆë””ì— "ë³´ì´ìŠ¤í”¼ì‹± ìœ„í—˜ì§•í›„ í¬ì°©"ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.
ëŒ€í™”ì— ìœ„í—˜ì§•í›„ê°€ ì—†ë‹¤ë©´ "íŠ¹ì´ì‚¬í•­ ì—†ìŒ" ìœ¼ë¡œë§Œ ëŒ€ë‹µí•˜ì„¸ìš”.
ëŒ€í™” ë°œí™”ì—ì„œ í¬ì°©í•œ ìœ„í—˜ ì§•í›„ê°€ ìˆì„ ê²½ìš°ì—ë§Œ í•´ë‹¹ ìœ„í—˜ì§•í›„ì— ëŒ€í•´ ê°„ëµí•˜ê²Œ ì„¤ëª…í•˜ê³  í–‰ë™ ê°€ì´ë“œë¥¼ ì•ˆë‚´í•˜ì„¸ìš”.
ì „ì²´ ëŒ€ë‹µì€ 3ì¤„ ì´í•˜ë¡œ ìš”ì•½í•˜ì—¬ í•´ì£¼ì„¸ìš”.

ìœ„í—˜ ì§•í›„:
- ê¸°ê´€ ì‚¬ì¹­: ê²½ì°°, ê²€ì°°, ì€í–‰ ë“± ê³µë¬´ì› ë° ê³µê³µê¸°ê´€ì„ ì‚¬ì¹­í•˜ë©° ê¸ˆì „ ìš”êµ¬
- ê¸ˆì•¡ ìš”êµ¬: ì•ˆì „ê³„ì¢Œ, ìˆ˜ìˆ˜ë£Œ, ì„¸ê¸ˆ ë“±ì„ ì´ìœ ë¡œ ì†¡ê¸ˆ ìš”êµ¬

í–‰ë™ ê°€ì´ë“œ:
- ì „í™”ë¡œ ìš”êµ¬ë°›ì€ ê³„ì¢Œë‚˜ ê¸ˆì•¡ì€ ì ˆëŒ€ ì´ì²´í•˜ì§€ ë§ˆì„¸ìš”.
- í•´ë‹¹ ê¸°ê´€ ê³µì‹ ëŒ€í‘œë²ˆí˜¸ë¡œ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.
- ì˜ì‹¬ë˜ë©´ ê²½ì°°ì„œë‚˜ ê¸ˆìœµê¸°ê´€ì— ì‹ ê³ í•˜ì„¸ìš”.

ëŒ€í™” ë°œí™”: ${segment.speaker}: ${segment.text}
â€» ${segment.speaker}ëŠ” ë°œí™”ì êµ¬ë¶„ìš©ì…ë‹ˆë‹¤.
`,
        token: API_TOKEN,
        history: historyRef.current, // ëˆ„ì  history ì „ì†¡
      };

      const res = await fetch(`${AI_SERVER_URL}/v1/chat`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(payload),
      });

      const data = await res.json();

      if (data.answer) {
        // âš ï¸ ìœ„í—˜ í‚¤ì›Œë“œ ë°œê²¬ ì‹œ ë³„ë„ UI/ë¡œê·¸ í‘œì‹œ
        if (/ê²½ê³ |ì£¼ì˜|ìœ„í—˜|ì‚¬ê¸°/.test(data.answer)) {
          logAI(
            `ğŸ¤– AI ë¶„ì„ ê²°ê³¼ \nì˜ì‹¬ë‚´ìš© = ${segment.speaker}: ${segment.text} \nğŸ›‘ ${data.answer} \n`
          );
        }
      }

      // 3ï¸âƒ£ STTë¥¼ historyì— ëˆ„ì 
      historyRef.current.push({
        role: "user",
        content: `[${segment.speaker}] ${segment.text}`,
      });

      // ì„ íƒ: AI ë‹µë³€ë„ historyì— ì¶”ê°€í•˜ë©´ ë‹¤ìŒ ìš”ì²­ì— ì»¨í…ìŠ¤íŠ¸ ë°˜ì˜ ê°€ëŠ¥
      historyRef.current.push({
        role: "assistant",
        content: data.answer || "",
      });
    } catch (err) {
      console.error("STT ì „ì†¡ ì‹¤íŒ¨", err);
    }
  };

  /** ---------------- Cleanup ---------------- */
  useEffect(() => {
    fetchRooms();
    return () => {
      handleMicStop();
      wsRef.current?.close();
      callWsRef.current?.close();
    };
  }, []);

  // ìœ ì € ì´ë©”ì¼ ì €ì¥
  useEffect(() => {
    const fetchUser = async () => {
      try {
        const me = await authService.me();
        setUserEmail(me.email); // emailì„ creatorId/userIdë¡œ ì‚¬ìš©
      } catch (err) {
        console.error("ì‚¬ìš©ì ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨", err);
      }
    };
    fetchUser();
  }, []);

  /** ---------------- UI ---------------- */
  const logsContainerRef = useRef<HTMLDivElement | null>(null);

  useEffect(() => {
    if (logsContainerRef.current) {
      logsContainerRef.current.scrollTop =
        logsContainerRef.current.scrollHeight;
    }
  }, [sttLogs]);

  const getLogClass = (text: string) => {
    if (/[ğŸ›‘]|ìœ„í—˜|ì—ëŸ¬|Error|âŒ/.test(text)) return "text-danger-600";
    if (/[âš ï¸]|ì£¼ì˜|ê²½ê³ /.test(text)) return "text-warning-600";
    if (/[âœ…]|connected|ì™„ë£Œ|success/i.test(text)) return "text-success-600";
    return "text-gray-800 dark:text-gray-200";
  };

  return (
    <div className="p-4 pb-4 h-full flex flex-col gap-4 bg-gradient-to-br from-gray-50 to-white dark:from-gray-900 dark:to-gray-950">
      {/* í†µí™”ë°© ì œì–´ */}
      <Card className="bg-white/70 dark:bg-gray-800/70 backdrop-blur border border-gray-100 dark:border-gray-700 overflow-visible">
        <div className="flex flex-wrap items-center justify-between gap-3 mb-3">
          <div className="flex items-center gap-2">
            <Button
              variant="primary"
              size="sm"
              className="whitespace-nowrap"
              onClick={createRoom}>
              í†µí™”ë°© ìƒì„±
            </Button>
            <span className="text-xs sm:text-sm text-gray-600 dark:text-gray-300">
              í˜„ì¬ í†µí™”ë°©:
              <span className="ml-1 px-2 py-0.5 rounded bg-gray-100 dark:bg-gray-700 text-gray-800 dark:text-gray-200">
                {currentRoom || "ì—†ìŒ"}
              </span>
            </span>
          </div>

          <div className="flex items-center gap-2">
            <Button variant="secondary" size="sm" onClick={connectWS}>
              AI ì—°ê²°
            </Button>
            <div className="relative">
              <input
                ref={fileInputRef}
                id="voice-file"
                type="file"
                accept="audio/*"
                onChange={handleFileUpload}
                className="hidden"
              />
              <Button variant="secondary" size="sm" onClick={triggerFileSelect}>
                ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ
              </Button>
            </div>
          </div>
        </div>

        <div className="mb-3">
          <h4 className="text-sm font-semibold mb-2 text-gray-800 dark:text-gray-100">
            í†µí™”ë°© ëª©ë¡
          </h4>
          <div className="flex gap-2 overflow-x-auto overflow-y-visible p-2 rounded-md [-ms-overflow-style:none] [scrollbar-width:none] [&::-webkit-scrollbar]:hidden">
            {rooms.length === 0 ? (
              <span className="text-xs text-gray-500 dark:text-gray-400">
                ìƒì„±ëœ ë°©ì´ ì—†ìŠµë‹ˆë‹¤.
              </span>
            ) : (
              rooms.map((room) => (
                <Button
                  key={room.id}
                  variant="secondary"
                  size="sm"
                  onClick={() => joinRoom(room.id)}
                  className={`${
                    currentRoom === room.id
                      ? "ring-inset ring-2 ring-primary-500 ring-offset-2 ring-offset-white dark:ring-offset-gray-900"
                      : ""
                  }`}>
                  {room.name} ({room.participants})
                </Button>
              ))
            )}
          </div>
        </div>

        {/* í†µí™” ì‹œì‘/ì¤‘ì§€ ë²„íŠ¼ì€ í•˜ë‹¨ ê³ ì • ì•¡ì…˜ë°”ì—ì„œë§Œ ì œê³µ */}
      </Card>

      {/* ë¡œê·¸ ë·° (íƒ­ ì „í™˜) */}
      <Card className="flex-1 flex flex-col overflow-hidden">
        {/* íƒ­ ë²„íŠ¼ */}
        <div className="flex border-b">
          <button
            onClick={() => setActiveTab("stt")}
            className={`flex-1 p-2 text-sm font-semibold ${
              activeTab === "stt"
                ? "border-b-2 border-blue-500 text-blue-600"
                : "text-gray-500"
            }`}>
            í†µí™” ë¡œê·¸
          </button>
          <button
            onClick={() => setActiveTab("ai")}
            className={`flex-1 p-2 text-sm font-semibold ${
              activeTab === "ai"
                ? "border-b-2 border-blue-500 text-blue-600"
                : "text-gray-500"
            }`}>
            ğŸ¤– AI ë¶„ì„ ë¡œê·¸
          </button>
        </div>

        {/* ë¡œê·¸ ë‚´ìš© */}
        <div className="flex-1 overflow-auto p-3 bg-gray-50">
          {activeTab === "stt"
            ? sttLogs.map((log, idx) => (
                <div
                  key={idx}
                  className="text-xs font-mono"
                  style={{ whiteSpace: "pre-wrap" }}>
                  {log}
                </div>
              ))
            : aiLogs.map((log, idx) => (
                <div
                  key={idx}
                  className="text-xs font-mono text-indigo-600"
                  style={{ whiteSpace: "pre-wrap" }}>
                  {log}
                </div>
              ))}
        </div>
      </Card>

      {/* í•˜ë‹¨ ì•¡ì…˜ë°”: ê³ ì • í•´ì œ, ìœ„ ë¸”ë¡ê³¼ 1rem ê°„ê²© ìœ ì§€ */}
      <div className="mt-4">
        <div className="w-full rounded-lg border border-gray-200 dark:border-gray-700 bg-white/70 dark:bg-gray-800/70 backdrop-blur shadow-xl px-4 py-3 overflow-hidden">
          <div className="flex flex-wrap items-center justify-between gap-3 gap-y-2">
            {/* ì˜¤ë””ì˜¤ ë°” ë¹„ì£¼ì–¼ë¼ì´ì € */}
            <div className="flex items-end gap-[3px] h-10 w-28 sm:w-40">
              {vizLevels.map((lv, i) => (
                <span
                  key={i}
                  className="w-[6px] rounded-full bg-gradient-to-b from-primary-500 to-indigo-500 dark:from-primary-400 dark:to-indigo-400 transition-[height] duration-75"
                  style={{ height: `${8 + Math.round(lv * 28)}px` }}
                />
              ))}
            </div>

            <div className="flex items-center gap-2 shrink-0">
              <span className="inline-flex items-center gap-1 px-2 py-1 rounded-full text-xs font-medium bg-gray-100 text-gray-700 dark:bg-gray-700 dark:text-gray-200 whitespace-nowrap">
                <span
                  className={`w-2 h-2 rounded-full ${
                    recordingRef.current ? "bg-red-500" : "bg-gray-400"
                  }`}
                />
                {recordingRef.current ? "ë…¹ìŒ ì¤‘" : "ëŒ€ê¸°"}
              </span>
              <span className="inline-flex items-center gap-1 px-2 py-1 rounded-full text-xs font-medium bg-gray-100 text-gray-700 dark:bg-gray-700 dark:text-gray-200 whitespace-nowrap">
                <span
                  className={`w-2 h-2 rounded-full ${
                    wsRef.current?.readyState === WebSocket.OPEN
                      ? "bg-green-500"
                      : "bg-gray-400"
                  }`}
                />
                {wsRef.current?.readyState === WebSocket.OPEN
                  ? "AI ì—°ê²°ë¨"
                  : "AI ë¯¸ì—°ê²°"}
              </span>
            </div>

            <div className="flex items-center gap-2 shrink-0 ml-auto">
              {!recordingRef.current ? (
                <Button
                  variant="primary"
                  size="sm"
                  onClick={() => {
                    if (currentRoom) {
                      connectCallWS(currentRoom);
                      handleMicStart();
                    }
                  }}
                  className="shadow-md whitespace-nowrap"
                  disabled={!currentRoom}>
                  í†µí™” ì‹œì‘
                </Button>
              ) : (
                <Button
                  variant="danger"
                  size="sm"
                  onClick={() => {
                    handleMicStop();
                    leaveRoom();
                  }}
                  className="shadow-md whitespace-nowrap">
                  í†µí™” ì¤‘ì§€
                </Button>
              )}
            </div>
          </div>
        </div>
      </div>
    </div>
  );
};

export default VoiceTab;
